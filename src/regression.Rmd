We first initialize the `tidyverse` library that contains `dplyr` and ``ggplot2`.
```{r}
library(tidyverse)
```

Let's import the data.
```{r}
setwd("/Users/mathieu/Lab/company-electricity-statistical-analysis/src/")
df <- read.csv2(file = "electricity_consumption_dataset.txt", sep = ";", dec = ".")
head(df)
```

This data contains 2 variables: **X: Electricity consumption (MWh)** and **Y = Productivity (1000€ / day)**.    
We rename the variables accordingly.

```{r}
df <- rename(df, electricity_consumption = X, productivity = Y)
```

## 1. Fitting naively a linear model.

We want to fit a linear model on our data. Is there a linear relationship between the **electricity consumption** and the **productivity** ? We can plot a _scatter plot_ in order to visualize the relationship between these 2 variables.
```{r}
ggplot(df, aes(x = electricity_consumption, y = productivity)) +
  geom_jitter() +
  geom_smooth() +
  ggtitle("Productivity ~ Electricity consumption") +
  xlab("Electricity consumption (MWh)") +
  ylab("Productivity (1000€ / day)")
```

As we can see, this does not seem linear at all. We can still try to fit a linear model.    
We use the **ordinary least square (OLS)** method that consists in minimising the sum of the square of the error: 
$$
\text{SSE} = \sum_{i = 0}^{+n} (y_i - \hat{y}_i)^2
$$

We can do that easily in r 
```{r}
simple_lm <- lm(productivity ~ electricity_consumption, data = df)
summary(simple_lm)
```

This method is based on **4 assumptions** :
- **Linearity**: the relationship between the variables X and Y is linear.    
- **Independence**: each observation is independent.    
- **Homoscedasticity**: the residuals have constant variance at every level.   
- **Normality**: the residuals are normaly distributed.

Let's check that out.

```{r}
par(mfrow = c(2,2))
plot(simple_lm)
```

### 1.1. Linearity

We can plot a _scatter plot_ of the **residuals** again the **fitted values**.    
If there is a linear relationship, the points should be randomly distributed around the horizontal line $y = 0$.

As we can see in the **residual vs fitted** plot, the mean of the residuals is not $O$.

### 1.2. Independence 

We want to make sure the is no dependance between some variables (i.e. each observation is collected independently). We can check the _correlation coefficient_ of 2 variables in r.

### 1.3. Homoescedasticity

We want to make sure the variance of the error term $\epsilon$ of the model is constant. The **Null Hypothesis $H_0$** is that our model is _homoscedastic. We don't want to reject that. We want to check that the **p-value** of the **Breush-Pagan test** is above the 5% level otherwise we should add more independant variables to our model that could explain why the error is larger in some cases and not others.

### 1.4. Normality 

We want to make sure the residuals are normaly distributed so that our model is not consistently under- or over-predicting the values.    
In order to visualize that in r, we can plot an _histogram_ of the residuals or a _qqplot_

We can see in the **qqplot** that the high and low values are pulling away from the dashed line.

## Model evaluation

We want to check if our model is reliable is a tool for understanding the phenomema we're studying.

### R-Squared 

The **R-Squared** is a number ranging from $0$ to $1$. It is commonly interpreted as indicating the **percentage of share of the variance in the dependent variable**.

Higher value of the **R-Squared** means that we explained more of the variation in our dependent variable.    
It should be noted that **R-Squared** can only increase as we add more variables but adding more variables does not automatically improve our model. Therefore, **Adjusted R-Squared** is a better metric in case of multiple linear regression as it decreases if the added variables do not improve the model.

```{r}
summary(simple_lm)
```

The **R-Squared** is $0.2614$ so $26%$ of the variation of the **productivity** is explained by the **electricity consumption**.

## 2. Transforming the variables

### 2.1. Square transformation

We first try improving our model by adding a square term. 
```{r}
square_lm <- lm(productivity ~ electricity_consumption + I(electricity_consumption^2), data = df)
summary(square_lm)

par(mfrow = c(2,2))
plot(square_lm)
```

```{r}
ggplot(df, aes(x = electricity_consumption, y = productivity)) +
  geom_point(alpha = 0.7) +
  geom_point(aes(x = electricity_consumption, y = square_lm$fitted.values), color = "red", alpha = 0.5) +
  geom_segment(aes(xend = electricity_consumption, yend = square_lm$fitted.values), color = "red", alpha = 0.25, linetype = "dashed")
```

```{r}
ggplot(df, aes(x = electricity_consumption, y = productivity)) + 
  geom_point(alpha = 0.75) +
  geom_smooth(method = "lm", formula = y ~ x + I(x^2), color = "red")
```

### 2.2. Log transformation 
```{r}
log_lm <- lm(productivity ~  log(electricity_consumption), data = df)
summary(log_lm)

par(mfrow = c(2,2))
plot(log_lm)
```

```{r}
ggplot(log_lm, aes(x = .resid)) +
  geom_histogram(binwidth = 25) +
  xlab("residuals")
```

### 2.2. Other Log transformation 
```{r}
other_log_lm <- lm(log(productivity) ~ electricity_consumption, data = df)
summary(log_lm)

par(mfrow = c(2,2))
plot(other_log_lm)
```

# checking normality of the residuals
```{r}
qqnorm(log_lm$residuals)
```

```{r}
ggplot(log_lm, aes(sample = .resid)) +
  stat_qq()
```

```{r}
ggplot(log_lm, aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_hline(yintercept = 0) +
  geom_smooth(se = FALSE)
```

```{r}
ggplot(df, aes(x = electricity_consumption, y = productivity)) +
  geom_point() +
  stat_smooth(method = "lm", formula = y ~ x + log2(x^2), se = FALSE)
```